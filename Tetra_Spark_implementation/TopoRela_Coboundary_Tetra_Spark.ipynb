{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "public-thesaurus",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "import os\n",
    "import gc\n",
    "#findspark.init() \n",
    "SPARK_HOME='/opt/cloudera/parcels/CDH/lib/spark'\n",
    "findspark.init(SPARK_HOME)\n",
    "\n",
    "import time\n",
    "import math\n",
    "import copy\n",
    "import csv\n",
    "import json\n",
    "import os\n",
    "import codecs\n",
    "import subprocess\n",
    "import numpy as np\n",
    "from pyspark import SQLContext\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.functions import size, array_union,flatten,array_sort,coalesce,broadcast,collect_list, collect_set, udf, array_remove, log, lit, first, col, array, sort_array,split, explode, desc, asc, row_number,isnan, when, count\n",
    "from pyspark.sql.types import *\n",
    "import rtree\n",
    "from pyspark.sql import Window\n",
    "import geofeather\n",
    "from pyspark.storagelevel import StorageLevel\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark import StorageLevel\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, LongType, FloatType, ArrayType, MapType\n",
    "from shapely.geometry import Point\n",
    "from shapely.geometry import Polygon\n",
    "\n",
    "from sedona.register import SedonaRegistrator\n",
    "from sedona.core.SpatialRDD import SpatialRDD, PointRDD, CircleRDD, PolygonRDD, LineStringRDD\n",
    "from sedona.core.enums import FileDataSplitter\n",
    "from sedona.utils.adapter import Adapter\n",
    "from sedona.core.spatialOperator import KNNQuery\n",
    "from sedona.core.spatialOperator import JoinQuery\n",
    "from sedona.core.spatialOperator import JoinQueryRaw\n",
    "from sedona.core.spatialOperator import RangeQuery\n",
    "from sedona.core.spatialOperator import RangeQueryRaw\n",
    "from sedona.core.formatMapper.shapefileParser import ShapefileReader\n",
    "from sedona.core.formatMapper import WkbReader\n",
    "from sedona.core.formatMapper import WktReader\n",
    "from sedona.core.formatMapper import GeoJsonReader\n",
    "from sedona.sql.types import GeometryType\n",
    "from sedona.core.enums import GridType\n",
    "from sedona.core.SpatialRDD import RectangleRDD\n",
    "from sedona.core.enums import IndexType\n",
    "from sedona.core.geom.envelope import Envelope\n",
    "from sedona.utils import SedonaKryoRegistrator, KryoSerializer\n",
    "\n",
    "os.environ['PYSPARK_PYTHON'] = \"./environment/bin/python\"\n",
    "#os.environ['PYSPARK_PYTHON'] = \"/home/qiany/.conda/envs/py37/bin/python\"\n",
    "os.environ['YARN_CONF_DIR'] = \"/opt/cloudera/parcels/CDH/lib/spark/conf/yarn-conf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "published-journalist",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "********************\n",
      "tin_directory:  /home/qiany/yuehui/pyspark/Tetra_mesh/data\n",
      "tin_basename:  Lander_big.ts\n",
      "tin_filename:  Lander_big\n",
      "tin_extension:  .ts\n",
      "\n",
      "********************\n",
      "This is a TIN file in \".ts\" format\n"
     ]
    }
   ],
   "source": [
    "tin_file = input(\"Here is a programe to extract boundary relations, please input the absolute or relative path to your .ts file:\")\n",
    "\n",
    "# get the directory, basename of the input file\n",
    "print(\"\\n********************\")\n",
    "tin_directory = os.path.dirname(tin_file)\n",
    "print(\"tin_directory: \", tin_directory)\n",
    "\n",
    "directory_type = input(\"Is the data stored in hdfs(0) or Tri_data(1) or Tetra_data (2):\") or \"2\"\n",
    "\n",
    "if directory_type == '0':\n",
    "    directory = 'hdfs_data'\n",
    "elif directory_type == '1':\n",
    "    directory = 'Tri_data'\n",
    "else:\n",
    "    directory = 'Tetra_data'\n",
    "    \n",
    "tin_basename = os.path.basename(tin_file) # input_vertices_2.off\n",
    "print(\"tin_basename: \", tin_basename)\n",
    "\n",
    "tin_filename = os.path.splitext(tin_basename)[0] # input_vertices_2\n",
    "print(\"tin_filename: \", tin_filename)\n",
    "\n",
    "tin_extension = os.path.splitext(tin_basename)[1] # .off\n",
    "print(\"tin_extension: \", tin_extension)\n",
    "\n",
    "print(\"\\n********************\")\n",
    "print(\"This is a TIN file in \\\"%s\\\" format\" % tin_extension)\n",
    "\n",
    "filtra = 'yes'\n",
    "\n",
    "# allocate the number of executors, the number of cores per executor, and the amount of memory per executor\n",
    "Num_executor = '64'\n",
    "Num_core_per_executor = '5'\n",
    "Memory_executor = '64g'\n",
    "MemoryOverhead_executor = '8g'\n",
    "\n",
    "# allocate the number of cores for the driver node\n",
    "Num_core_per_driver = '5'\n",
    "Memory_driver = '64g'\n",
    "MemoryOverhead_driver = '32g'\n",
    "\n",
    "# the default number of shuffle partitions\n",
    "Num_shuffle_partitions = input(\"spark.sql.shuffle.partitions:\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "atomic-citizenship",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "What kind of connectivity relation do you want to retrieve? EF_M4_show\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spark_app_name: TopoRela_CoBoundary_TetraMesh_Lander_big_06142024_1558_EF_M4_show\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "spark.executor.cores: # Number of concurrent tasks an executor can run, euqals to the number of cores to use on each executor\n",
    "spark.executor.instances: # Number of executors for the spark application\n",
    "spark.executor.memory: # Amount of memory to use for each executor that runs the task\n",
    "spark.executor.memoryOverhead:\n",
    "spark.driver.cores: # Number of cores to use for the driver process; the default number is 1\n",
    "spark.driver.memory: # Amount of memory to use for the driver\n",
    "spark.driver.maxResultSize: to define the maximum limit of the total size of the serialized result that a driver can store for each Spark collect action\n",
    "spark.default.parallelism: # Default number of partitions in RDDs returned by transformations like join, reduceByKey, and parallelize when not set by user. It can be set as spark.executor.instances * spark.executor.cores * 2\n",
    "spark.sql.shuffle.partitions: determine how many partitions are used when data is shuffled between nodes, e.g., joins or aggregations. usually 1~5 times of executor.instances * executor.cores\n",
    "spark.memory.storageFraction: determines the fraction of the heap space that is allocated to caching RDDs and DataFrames in memory.\n",
    "spark.kryoserializer.buffer.max: determine the maximum of data that can be serialized at once; this must be larger than any object we attempt to serialize\n",
    "spark.rpc.message.maxSize: # Maximum message size (in MiB) to allow in \"control plane\" communication; generally only applies to map output size information sent between executors and the driver. To communicate between the nodes, Spark uses a protocol called RPC (Remote Procedure Call), which sends messages back and forth. The spark.rpc.message.maxSize parameter limits how big these messages can be. \n",
    "spark.sql.broadcastTimeout: Spark will wait for this amount of time before giving up on broadcasting a table. Broadcasting can take a long time if the table is large or if there is a shuffle operation before it.\n",
    "spark.sql.autoBroadcastJoinThreshold: Spark will broadcast a table to all worker nodes when performing a join if its size is less than this value; -1 means disabling broadcasting\n",
    "'''\n",
    "\n",
    "date = time.strftime(\"%m,%d,%Y\")\n",
    "date_name = date.split(',')[0] + date.split(',')[1] + date.split(',')[2]\n",
    "\n",
    "hour = time.strftime(\"%H,%M\")\n",
    "hour_name = hour.split(',')[0] + hour.split(',')[1]\n",
    "\n",
    "Topo = input(\"What kind of connectivity relation do you want to retrieve?\")\n",
    "\n",
    "spark_app_name = \"TopoRela_CoBoundary_TetraMesh_\" + tin_filename + '_' + date_name + '_' + hour_name + '_' + Topo\n",
    "print(\"spark_app_name:\", spark_app_name)\n",
    "\n",
    "spark = SparkSession \\\n",
    ".builder \\\n",
    ".appName(spark_app_name) \\\n",
    ".master('yarn') \\\n",
    ".config(\"spark.serializer\", KryoSerializer.getName) \\\n",
    ".config('spark.jars','sedona-core-2.4_2.11-1.0.0-incubating.jar,sedona-sql-2.4_2.11-1.0.0-incubating.jar,sedona-python-adapter-2.4_2.11-1.0.0-incubating.jar,sedona-viz-2.4_2.11-1.0.0-incubating.jar,geotools-wrapper-geotools-24.0.jar,graphframes-0.8.0-spark2.4-s_2.11.jar') \\\n",
    ".config('spark.executor.cores', Num_core_per_executor) \\\n",
    ".config('spark.executor.instances', Num_executor) \\\n",
    ".config('spark.executor.memory', Memory_executor) \\\n",
    ".config('spark.executor.memoryOverhead', MemoryOverhead_executor) \\\n",
    ".config('spark.driver.cores', Num_core_per_driver) \\\n",
    ".config('spark.driver.memory', Memory_driver) \\\n",
    ".config('spark.driver.memoryOverhead', MemoryOverhead_driver) \\\n",
    ".config('spark.driver.maxResultSize', '0') \\\n",
    ".config('spark.dynamicAllocation.enabled', 'false') \\\n",
    ".config('spark.network.timeout', '10000001s') \\\n",
    ".config('spark.executor.heartbeatInterval', '10000000s') \\\n",
    ".config('spark.sql.shuffle.partitions', Num_shuffle_partitions) \\\n",
    ".config(\"spark.default.parallelism\", '400') \\\n",
    ".config(\"spark.kryoserializer.buffer.max\", \"1024mb\") \\\n",
    ".config('spark.rpc.message.maxSize', '256') \\\n",
    ".config(\"spark.sql.broadcastTimeout\", \"36000\") \\\n",
    ".config(\"spark.sql.autoBroadcastJoinThreshold\", \"-1\") \\\n",
    ".config(\"spark.sql.objectHashAggregate.sortBased.fallbackThreshold\", \"-1\") \\\n",
    ".config('spark.yarn.dist.archives', '/local/data/yuehui/py37.tar.gz#environment') \\\n",
    ".config(\"spark.python.profile\", \"true\") \\\n",
    ".config(\"spark.eventLog.enabled\", \"true\") \\\n",
    ".config(\"spark.eventLog.logStageExecutorMetrics\", \"true\") \\\n",
    ".getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "laden-eligibility",
   "metadata": {},
   "source": [
    "### read input files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "attempted-mercury",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of partitions for df_tetra_origin: 4959\n",
      "root\n",
      " |-- r1: integer (nullable = true)\n",
      " |-- r2: integer (nullable = true)\n",
      " |-- r3: integer (nullable = true)\n",
      " |-- r4: integer (nullable = true)\n",
      " |-- r1_ele: float (nullable = true)\n",
      " |-- r2_ele: float (nullable = true)\n",
      " |-- r3_ele: float (nullable = true)\n",
      " |-- r4_ele: float (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# read_tetra_order() is a function used to read tetrahedron from a csv file\n",
    "def read_tetra_order(hdfs_tetra_origin):\n",
    "    '''\n",
    "    this function has two input parameters.\n",
    "    filtra: 'yes' or 'no', yes means that the input csv file is ordered by default\n",
    "    directory: a string denoting the directory to a tetrahedra file\n",
    "    tin_filename: a string denoting the file name of a tetrahedra extension, e.g., 827_monviso\n",
    "    '''\n",
    "        \n",
    "    schema_tetra_origin = StructType([ \\\n",
    "        StructField(\"tetra_order\",IntegerType(),True), \\\n",
    "        StructField(\"r1\",IntegerType(),True), \\\n",
    "        StructField(\"r2\",IntegerType(),True), \\\n",
    "        StructField(\"r3\",IntegerType(),True), \\\n",
    "        StructField(\"r4\",IntegerType(),True), \\\n",
    "        StructField(\"r1_ele\",FloatType(),True), \\\n",
    "        StructField(\"r2_ele\",FloatType(),True), \\\n",
    "        StructField(\"r3_ele\",FloatType(),True), \\\n",
    "        StructField(\"r4_ele\",FloatType(),True) \\\n",
    "      ])\n",
    "\n",
    "    df_tetra_origin = spark.read.format(\"csv\") \\\n",
    "          .option(\"header\", False) \\\n",
    "          .schema(schema_tetra_origin)\\\n",
    "          .load(hdfs_tetra_origin)\n",
    "        \n",
    "    return df_tetra_origin\n",
    "\n",
    "\n",
    "# read tetrahedra\n",
    "hdfs_tetra_origin = directory + \"/\" + tin_filename + '_filtra_tetra_sort.csv'\n",
    "\n",
    "df_tetra_order = read_tetra_order(hdfs_tetra_origin)\n",
    "df_tetra_order.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "healthy-socket",
   "metadata": {},
   "source": [
    "# obtain VT relation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "contrary-controversy",
   "metadata": {},
   "source": [
    "### Method 1: pure global"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "wired-shelter",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to get VT directly from DF_T\n",
    "def get_VT(df_tetra_order):\n",
    "    df_tetra_order = df_tetra_order.withColumn(\"tetra\", sort_array(F.array(\"r1\", \"r2\", \"r3\", \"r4\"), False))\n",
    "    df_VT_init_1 = df_tetra_order.select(\"r1\",\"tetra\")\n",
    "    df_VT_init_2 = df_tetra_order.select(\"r2\",\"tetra\")\n",
    "    df_VT_init_3 = df_tetra_order.select(\"r3\",\"tetra\")\n",
    "    df_VT_init_4 = df_tetra_order.select(\"r4\",\"tetra\")\n",
    "    \n",
    "    df_VT_union12 = df_VT_init_1.union(df_VT_init_2)\n",
    "    df_VT_union123 = df_VT_union12.union(df_VT_init_3)\n",
    "    df_VT_union1234 = df_VT_union123.union(df_VT_init_4)\n",
    "    \n",
    "    df_VT = df_VT_union1234.groupBy('r1').agg(collect_list('tetra').alias('VT'))\n",
    "    df_VT = df_VT.withColumnRenamed('r1', 'Ver')\n",
    "    \n",
    "    return df_VT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "developing-internship",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Ver: integer (nullable = true)\n",
      " |-- VT: array (nullable = true)\n",
      " |    |-- element: array (containsNull = true)\n",
      " |    |    |-- element: integer (containsNull = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_VT = get_VT(df_tetra_order)\n",
    "\n",
    "df_VT.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "accepting-strike",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time cost: 498.6207056045532\n",
      "number of rows: 1792989718\n"
     ]
    }
   ],
   "source": [
    "t0 = time.time()\n",
    "\n",
    "num_row = df_VT.count()\n",
    "\n",
    "t1 = time.time()\n",
    "print(\"Time cost:\", t1 - t0)\n",
    "print(\"number of rows:\", num_row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "secure-defeat",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time cost: 1447.6604726314545\n"
     ]
    }
   ],
   "source": [
    "t0 = time.time()\n",
    "\n",
    "file_VT = directory + '/' + tin_filename + '_VT.parquet'\n",
    "\n",
    "df_VT.write.parquet(file_VT)\n",
    "\n",
    "t1 = time.time()\n",
    "print(\"time cost:\", t1-t0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hydraulic-round",
   "metadata": {},
   "source": [
    "# obtain VF relation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fifty-quality",
   "metadata": {},
   "source": [
    "### Method 1: pure global"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "accredited-depth",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- r1: integer (nullable = true)\n",
      " |-- multi_f1: array (nullable = true)\n",
      " |    |-- element: array (containsNull = true)\n",
      " |    |    |-- element: integer (containsNull = true)\n",
      " |-- multi_f2: array (nullable = true)\n",
      " |    |-- element: array (containsNull = true)\n",
      " |    |    |-- element: integer (containsNull = true)\n",
      " |-- multi_f3: array (nullable = true)\n",
      " |    |-- element: array (containsNull = true)\n",
      " |    |    |-- element: integer (containsNull = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# function to get VF from DF_T\n",
    "def get_VF_init(df_tetra_order):\n",
    "    df_VF_1 = df_tetra_order.withColumn(\"f1\", sort_array(F.array(\"r1\", \"r2\", \"r3\"), False)).withColumn(\"f2\", sort_array(F.array(\"r1\", \"r2\", \"r4\"), False)).withColumn(\"f3\", sort_array(F.array(\"r1\", \"r3\", \"r4\"), False)).drop('r2', 'r3', 'r4')\n",
    "    df_VF_2 = df_tetra_order.withColumn(\"f1\", sort_array(F.array(\"r2\", \"r1\", \"r3\"), False)).withColumn(\"f2\", sort_array(F.array(\"r2\", \"r1\", \"r4\"), False)).withColumn(\"f3\", sort_array(F.array(\"r2\", \"r3\", \"r4\"), False)).drop('r1', 'r3', 'r4')\n",
    "    df_VF_3 = df_tetra_order.withColumn(\"f1\", sort_array(F.array(\"r3\", \"r1\", \"r2\"), False)).withColumn(\"f2\", sort_array(F.array(\"r3\", \"r1\", \"r4\"), False)).withColumn(\"f3\", sort_array(F.array(\"r3\", \"r2\", \"r4\"), False)).drop('r1', 'r2', 'r4')\n",
    "    df_VF_4 = df_tetra_order.withColumn(\"f1\", sort_array(F.array(\"r4\", \"r1\", \"r2\"), False)).withColumn(\"f2\", sort_array(F.array(\"r4\", \"r1\", \"r3\"), False)).withColumn(\"f3\", sort_array(F.array(\"r4\", \"r2\", \"r3\"), False)).drop('r1', 'r2', 'r3')\n",
    "    \n",
    "    df_VF_union12 = df_VF_1.union(df_VF_2)\n",
    "    df_VF_union123 = df_VF_union12.union(df_VF_3)\n",
    "    df_VF_union1234 = df_VF_union123.union(df_VF_4)\n",
    "    \n",
    "    df_VF_init = df_VF_union1234.groupBy('r1').agg(collect_set('f1').alias('multi_f1'), collect_set('f2').alias('multi_f2'), collect_set('f3').alias('multi_f3'))\n",
    "    return df_VF_init\n",
    "\n",
    "df_VF_init = get_VF_init(df_tetra_order)\n",
    "\n",
    "# df_VE_init.cache()\n",
    "\n",
    "df_VF_init.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "specialized-motorcycle",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Ver: integer (nullable = true)\n",
      " |-- VF: array (nullable = true)\n",
      " |    |-- element: array (containsNull = true)\n",
      " |    |    |-- element: integer (containsNull = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# obtain VF relation\n",
    "def get_VF(multi_f1, multi_f2, multi_f3):\n",
    "# get_VF is used to obtain a complete VF relation from the partial VF relations\n",
    "# multi_f1: partial VF relation\n",
    "# multi_f2: partial VF relation\n",
    "# multi_f3: partial VF relation\n",
    "\n",
    "    faces = set()\n",
    "    for f in multi_f1:\n",
    "        faces.add(tuple(f))\n",
    "        \n",
    "    for f in multi_f2:\n",
    "        faces.add(tuple(f))\n",
    "        \n",
    "    for f in multi_f3:\n",
    "        faces.add(tuple(f))\n",
    "    \n",
    "    faces_list = sorted(faces) # save more time when using list(faces)\n",
    "    \n",
    "    return faces_list\n",
    "\n",
    "# convert a function to an udf and determine the return type\n",
    "# https://spark.apache.org/docs/3.1.3/api/python/reference/api/pyspark.sql.functions.udf.html\n",
    "get_VF_udf = udf(get_VF, ArrayType(ArrayType(IntegerType())))\n",
    "\n",
    "df_VF = df_VF_init.withColumn(\"VF\", get_VF_udf(df_VF_init.multi_f1, df_VF_init.multi_f2, df_VF_init.multi_f3)).drop('multi_f1', 'multi_f2', 'multi_f3')\n",
    "df_VF = df_VF.withColumnRenamed('r1', 'Ver')\n",
    "\n",
    "df_VF.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "third-buffer",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time cost: 565.2756164073944\n",
      "number of rows: 1792989718\n"
     ]
    }
   ],
   "source": [
    "t0 = time.time()\n",
    "\n",
    "num_row = df_VF.count()\n",
    "\n",
    "t1 = time.time()\n",
    "print(\"Time cost:\", t1 - t0)\n",
    "print(\"number of rows:\", num_row)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "boring-portrait",
   "metadata": {},
   "source": [
    "### Method 2: symmetric global (get VF from FV)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unexpected-salmon",
   "metadata": {},
   "source": [
    "##### load the pre-computed FV relation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dedicated-security",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- face: array (nullable = true)\n",
      " |    |-- element: integer (containsNull = true)\n",
      " |-- r1: integer (nullable = true)\n",
      " |-- r2: integer (nullable = true)\n",
      " |-- r3: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# read a parquet file from hdfs\n",
    "file_FV = directory + '/' + tin_filename + '_FV.parquet'\n",
    "\n",
    "df_FV = spark.read.parquet(file_FV)\n",
    "df_FV.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cardiovascular-jacket",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time cost: 18.51378083229065\n",
      "number of rows: 19434269798\n"
     ]
    }
   ],
   "source": [
    "t0 = time.time()\n",
    "\n",
    "num_row = df_FV.count()\n",
    "\n",
    "t1 = time.time()\n",
    "print(\"Time cost:\", t1 - t0)\n",
    "print(\"number of rows:\", num_row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "historical-translation",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Ver: integer (nullable = true)\n",
      " |-- VF: array (nullable = true)\n",
      " |    |-- element: array (containsNull = true)\n",
      " |    |    |-- element: integer (containsNull = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_VF_1 = df_FV.select('r1', 'face')\n",
    "df_VF_2 = df_FV.select('r2', 'face')\n",
    "df_VF_3 = df_FV.select('r3', 'face')\n",
    "\n",
    "df_VF_union = df_VF_1.union(df_VF_2).union(df_VF_3)\n",
    "df_VF = df_VF_union.groupBy('r1').agg(collect_list('face').alias('VF'))\n",
    "df_VF = df_VF.withColumnRenamed('r1', 'Ver')\n",
    "df_VF.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "mature-mobile",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time cost: 441.9637041091919\n",
      "number of rows: 1792989718\n"
     ]
    }
   ],
   "source": [
    "t0 = time.time()\n",
    "\n",
    "num_row = df_VF.count()\n",
    "\n",
    "t1 = time.time()\n",
    "print(\"Time cost:\", t1 - t0)\n",
    "print(\"number of rows:\", num_row)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "enhanced-ridge",
   "metadata": {},
   "source": [
    "### Method 3: local method (get VF from VT)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "liable-marking",
   "metadata": {},
   "source": [
    "##### load the pre-computed VT relation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "distinguished-racing",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read a parquet file from hdfs\n",
    "file_VT = directory + '/' + tin_filename + '_VT.parquet'\n",
    "\n",
    "df_VT = spark.read.parquet(file_VT)\n",
    "df_VT.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "collective-sensitivity",
   "metadata": {},
   "outputs": [],
   "source": [
    "t0 = time.time()\n",
    "\n",
    "num_row = df_VT.count()\n",
    "\n",
    "t1 = time.time()\n",
    "print(\"Time cost:\", t1 - t0)\n",
    "# print(\"number of rows:\", num_row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efficient-fruit",
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import combinations\n",
    "def get_VF_from_VT(Ver, VT):\n",
    "    if VT:\n",
    "        multi_F = set()\n",
    "        for tretra in VT:\n",
    "            # Generate combinations of 3 elements including Ver\n",
    "            for comb in combinations(tretra, 3):\n",
    "                if Ver in comb:\n",
    "                    sorted_comb = sorted(comb, reverse=True)\n",
    "                    multi_F.add(tuple(sorted_comb))\n",
    "                    \n",
    "        multi_F_list = sorted(multi_F) # save more time when using list(multi_F)\n",
    "            \n",
    "        return multi_F_list\n",
    "             \n",
    "get_VF_from_VT_udf = udf(get_VF_from_VT, ArrayType(ArrayType(IntegerType())))\n",
    "\n",
    "df_VF = df_VT.withColumn(\"VF\", get_VF_from_VT_udf(df_VT.Ver, df_VT.VT))\n",
    "df_VF = df_VF.select(\"Ver\", \"VF\")\n",
    "df_VF.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "statewide-monkey",
   "metadata": {},
   "outputs": [],
   "source": [
    "t0 = time.time()\n",
    "\n",
    "num_row = df_VF.count()\n",
    "# df_VF.show()\n",
    "\n",
    "t1 = time.time()\n",
    "print(\"Time cost:\", t1 - t0)\n",
    "# print(\"number of rows:\", num_row)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "underlying-yemen",
   "metadata": {},
   "source": [
    "# obtain VE relation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "jewish-realtor",
   "metadata": {},
   "source": [
    "### Method 1: pure global"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "appointed-organic",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- r1: integer (nullable = true)\n",
      " |-- multi_e1: array (nullable = true)\n",
      " |    |-- element: array (containsNull = true)\n",
      " |    |    |-- element: integer (containsNull = true)\n",
      " |-- multi_e2: array (nullable = true)\n",
      " |    |-- element: array (containsNull = true)\n",
      " |    |    |-- element: integer (containsNull = true)\n",
      " |-- multi_e3: array (nullable = true)\n",
      " |    |-- element: array (containsNull = true)\n",
      " |    |    |-- element: integer (containsNull = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# function to get VE from DF_T\n",
    "def get_VE_init(df_tetra_order):\n",
    "    df_VE_1 = df_tetra_order.withColumn(\"e1\", sort_array(F.array(\"r1\", \"r2\"), False)).withColumn(\"e2\", sort_array(F.array(\"r1\", \"r3\"), False)).withColumn(\"e3\", sort_array(F.array(\"r1\", \"r4\"), False)).drop('r2', 'r3', 'r4')\n",
    "    df_VE_2 = df_tetra_order.withColumn(\"e1\", sort_array(F.array(\"r2\", \"r1\"), False)).withColumn(\"e2\", sort_array(F.array(\"r2\", \"r3\"), False)).withColumn(\"e3\", sort_array(F.array(\"r2\", \"r4\"), False)).drop('r1', 'r3', 'r4')\n",
    "    df_VE_3 = df_tetra_order.withColumn(\"e1\", sort_array(F.array(\"r3\", \"r1\"), False)).withColumn(\"e2\", sort_array(F.array(\"r3\", \"r2\"), False)).withColumn(\"e3\", sort_array(F.array(\"r3\", \"r4\"), False)).drop('r1', 'r2', 'r4')\n",
    "    df_VE_4 = df_tetra_order.withColumn(\"e1\", sort_array(F.array(\"r4\", \"r1\"), False)).withColumn(\"e2\", sort_array(F.array(\"r4\", \"r2\"), False)).withColumn(\"e3\", sort_array(F.array(\"r4\", \"r3\"), False)).drop('r1', 'r2', 'r3')\n",
    "    \n",
    "    df_VE_union1234 = df_VE_1.union(df_VE_2).union(df_VE_3).union(df_VE_4)\n",
    "    \n",
    "    df_VE_init = df_VE_union1234.groupBy('r1').agg(collect_set('e1').alias('multi_e1'), collect_set('e2').alias('multi_e2'), collect_set('e3').alias('multi_e3'))\n",
    "    return df_VE_init\n",
    "\n",
    "df_VE_init = get_VE_init(df_tetra_order)\n",
    "\n",
    "# df_VE_init.cache()\n",
    "\n",
    "df_VE_init.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "conservative-independence",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Ver: integer (nullable = true)\n",
      " |-- VE: array (nullable = true)\n",
      " |    |-- element: array (containsNull = true)\n",
      " |    |    |-- element: integer (containsNull = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# obtain VE relation\n",
    "def get_VE(multi_e1, multi_e2, multi_e3):\n",
    "# get_VE is used to obtain the partial VE relation\n",
    "# multi_e1: partial VE relation\n",
    "# multi_e2: partial VE relation\n",
    "    edges = set()\n",
    "    for e in multi_e1:\n",
    "        edges.add(tuple(e))\n",
    "        \n",
    "    for e in multi_e2:\n",
    "        edges.add(tuple(e))\n",
    "        \n",
    "    for e in multi_e3:\n",
    "        edges.add(tuple(e))\n",
    "    \n",
    "    edges_list = sorted(edges) # sorted(pt_set, reverse=False), False in ascending order while True in descending order\n",
    "    \n",
    "    return edges_list\n",
    "\n",
    "# convert a function to an udf and determine the return type\n",
    "# https://spark.apache.org/docs/3.1.3/api/python/reference/api/pyspark.sql.functions.udf.html\n",
    "get_VE_udf = udf(get_VE, ArrayType(ArrayType(IntegerType())))\n",
    "\n",
    "df_VE = df_VE_init.withColumn(\"VE\", get_VE_udf(df_VE_init.multi_e1, df_VE_init.multi_e2, df_VE_init.multi_e3)).drop('multi_e1', 'multi_e2', 'multi_e3')\n",
    "df_VE = df_VE.withColumnRenamed('r1', 'Ver')\n",
    "\n",
    "df_VE.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "finnish-recommendation",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time cost: 501.4733638763428\n",
      "number of rows: 1792989718\n"
     ]
    }
   ],
   "source": [
    "t0 = time.time()\n",
    "\n",
    "num_row = df_VE.count()\n",
    "\n",
    "t1 = time.time()\n",
    "print(\"Time cost:\", t1 - t0)\n",
    "print(\"number of rows:\", num_row)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stunning-jewel",
   "metadata": {},
   "source": [
    "### Method 2: symmetric global (get VE from EV)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daily-convergence",
   "metadata": {},
   "source": [
    "##### load the pre-computed EV relation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hispanic-convergence",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read a parquet file from hdfs\n",
    "file_EV = directory + '/' + tin_filename + '_EV.parquet'\n",
    "\n",
    "df_EV = spark.read.parquet(file_EV)\n",
    "df_EV.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "incorrect-prescription",
   "metadata": {},
   "outputs": [],
   "source": [
    "t0 = time.time()\n",
    "\n",
    "num_row = df_EV.count()\n",
    "\n",
    "t1 = time.time()\n",
    "print(\"Time cost:\", t1 - t0)\n",
    "print(\"number of rows:\", num_row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "martial-parks",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_VE_1 = df_EV.select('r1', 'edge')\n",
    "df_VE_2 = df_EV.select('r2', 'edge')\n",
    "\n",
    "df_VE_union = df_VE_1.union(df_VE_2)\n",
    "df_VE = df_VE_union.groupBy('r1').agg(collect_list('edge').alias('VE'))\n",
    "df_VE = df_VE.withColumnRenamed('r1', 'Ver')\n",
    "df_VE.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hollywood-investor",
   "metadata": {},
   "outputs": [],
   "source": [
    "t0 = time.time()\n",
    "\n",
    "num_row = df_VE.count()\n",
    "\n",
    "t1 = time.time()\n",
    "print(\"Time cost:\", t1 - t0)\n",
    "print(\"number of rows:\", num_row)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "raised-response",
   "metadata": {},
   "source": [
    "### Method 3: local method (get VE from VT)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "extended-survival",
   "metadata": {},
   "source": [
    "##### load the pre-computed VT relation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "responsible-tourist",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Ver: integer (nullable = true)\n",
      " |-- VT: array (nullable = true)\n",
      " |    |-- element: array (containsNull = true)\n",
      " |    |    |-- element: integer (containsNull = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# read a parquet file from hdfs\n",
    "file_VT = directory + '/' + tin_filename + '_VT.parquet'\n",
    "\n",
    "df_VT = spark.read.parquet(file_VT)\n",
    "df_VT.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bizarre-trailer",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time cost: 15.57933759689331\n",
      "number of rows: 1792989718\n"
     ]
    }
   ],
   "source": [
    "t0 = time.time()\n",
    "\n",
    "num_row = df_VT.count()\n",
    "\n",
    "t1 = time.time()\n",
    "print(\"Time cost:\", t1 - t0)\n",
    "print(\"number of rows:\", num_row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "chinese-deputy",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Ver: integer (nullable = true)\n",
      " |-- VE: array (nullable = true)\n",
      " |    |-- element: array (containsNull = true)\n",
      " |    |    |-- element: integer (containsNull = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def get_VE_from_VT(Ver, VT):\n",
    "    if VT:\n",
    "        multi_E = set()\n",
    "        for tretra in VT:\n",
    "            for v in tretra:\n",
    "                if v > Ver:\n",
    "                    multi_E.add(tuple([v, Ver]))\n",
    "                elif v < Ver:\n",
    "                    multi_E.add(tuple([Ver, v]))\n",
    "                # ignore if v == Ver\n",
    "            \n",
    "        return sorted(multi_E)\n",
    "             \n",
    "get_VE_from_VT_udf = udf(get_VE_from_VT, ArrayType(ArrayType(IntegerType())))\n",
    "\n",
    "df_VE = df_VT.withColumn(\"VE\", get_VE_from_VT_udf(df_VT.Ver, df_VT.VT))\n",
    "df_VE = df_VE.select(\"Ver\", \"VE\")\n",
    "df_VE.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "spanish-yugoslavia",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time cost: 6.331205129623413\n",
      "number of rows: 1792989718\n"
     ]
    }
   ],
   "source": [
    "t0 = time.time()\n",
    "\n",
    "num_row = df_VE.count()\n",
    "\n",
    "t1 = time.time()\n",
    "print(\"Time cost:\", t1 - t0)\n",
    "print(\"number of rows:\", num_row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "considerable-peeing",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time cost: 1007.657793045044\n"
     ]
    }
   ],
   "source": [
    "t0 = time.time()\n",
    "\n",
    "file_VE = directory + '/' + tin_filename + '_VE.parquet'\n",
    "df_VE.write.parquet(file_VE)\n",
    "\n",
    "t1 = time.time()\n",
    "print(\"Time cost:\", t1 - t0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "indonesian-print",
   "metadata": {},
   "source": [
    "# obtain EF relation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "likely-aircraft",
   "metadata": {},
   "source": [
    "### Method 1: pure global"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "relative-induction",
   "metadata": {},
   "source": [
    "##### first get EV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "injured-commerce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to get TE relation\n",
    "def get_TE_from_T(df_tetra_order):\n",
    "    df_TE = df_tetra_order.withColumn(\"tetra\", sort_array(F.array(\"r1\", \"r2\", \"r3\", \"r4\"), False)).withColumn(\"e1\", sort_array(F.array(\"r1\", \"r2\",), False)).withColumn(\"e2\", sort_array(F.array(\"r1\", \"r3\",), False)).withColumn(\"e3\", sort_array(F.array(\"r1\", \"r4\",), False)).withColumn(\"e4\", sort_array(F.array(\"r2\", \"r3\",), False)).withColumn(\"e5\", sort_array(F.array(\"r2\", \"r4\",), False)).withColumn(\"e6\", sort_array(F.array(\"r3\", \"r4\",), False))\n",
    "    df_TE = df_TE.select('tetra', 'e1', 'e2', 'e3', 'e4', 'e5', 'e6')\n",
    "    \n",
    "    return df_TE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "abstract-species",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- edge: array (nullable = false)\n",
      " |    |-- element: integer (containsNull = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# get edges from DF_T\n",
    "\n",
    "df_TE = get_TE_from_T(df_tetra_order)\n",
    "\n",
    "df_E1 = df_TE.select('e1')\n",
    "df_E2 = df_TE.select('e2')\n",
    "df_E3 = df_TE.select('e3')\n",
    "df_E4 = df_TE.select('e4')\n",
    "df_E5 = df_TE.select('e5')\n",
    "df_E6 = df_TE.select('e6')\n",
    "\n",
    "df_Edges = df_E1.union(df_E2).union(df_E3).union(df_E4).union(df_E5).union(df_E6)\n",
    "df_Edges = df_Edges.withColumnRenamed('e1', 'edge')\n",
    "df_Edges = df_Edges.distinct()\n",
    "df_Edges.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "favorite-preference",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- edge: array (nullable = false)\n",
      " |    |-- element: integer (containsNull = true)\n",
      " |-- r1: integer (nullable = true)\n",
      " |-- r2: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_EV = df_Edges.withColumn('r1', df_Edges.edge[0]).withColumn('r2', df_Edges.edge[1])\n",
    "df_EV.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rental-inflation",
   "metadata": {},
   "source": [
    "##### then get VF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "convenient-favorite",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- r1: integer (nullable = true)\n",
      " |-- multi_f1: array (nullable = true)\n",
      " |    |-- element: array (containsNull = true)\n",
      " |    |    |-- element: integer (containsNull = true)\n",
      " |-- multi_f2: array (nullable = true)\n",
      " |    |-- element: array (containsNull = true)\n",
      " |    |    |-- element: integer (containsNull = true)\n",
      " |-- multi_f3: array (nullable = true)\n",
      " |    |-- element: array (containsNull = true)\n",
      " |    |    |-- element: integer (containsNull = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# function to get VF from DF_T\n",
    "def get_VF_init(df_tetra_order):\n",
    "    df_VF_1 = df_tetra_order.withColumn(\"f1\", sort_array(F.array(\"r1\", \"r2\", \"r3\"), False)).withColumn(\"f2\", sort_array(F.array(\"r1\", \"r2\", \"r4\"), False)).withColumn(\"f3\", sort_array(F.array(\"r1\", \"r3\", \"r4\"), False)).drop('r2', 'r3', 'r4')\n",
    "    df_VF_2 = df_tetra_order.withColumn(\"f1\", sort_array(F.array(\"r2\", \"r1\", \"r3\"), False)).withColumn(\"f2\", sort_array(F.array(\"r2\", \"r1\", \"r4\"), False)).withColumn(\"f3\", sort_array(F.array(\"r2\", \"r3\", \"r4\"), False)).drop('r1', 'r3', 'r4')\n",
    "    df_VF_3 = df_tetra_order.withColumn(\"f1\", sort_array(F.array(\"r3\", \"r1\", \"r2\"), False)).withColumn(\"f2\", sort_array(F.array(\"r3\", \"r1\", \"r4\"), False)).withColumn(\"f3\", sort_array(F.array(\"r3\", \"r2\", \"r4\"), False)).drop('r1', 'r2', 'r4')\n",
    "    df_VF_4 = df_tetra_order.withColumn(\"f1\", sort_array(F.array(\"r4\", \"r1\", \"r2\"), False)).withColumn(\"f2\", sort_array(F.array(\"r4\", \"r1\", \"r3\"), False)).withColumn(\"f3\", sort_array(F.array(\"r4\", \"r2\", \"r3\"), False)).drop('r1', 'r2', 'r3')\n",
    "    \n",
    "    df_VF_union12 = df_VF_1.union(df_VF_2)\n",
    "    df_VF_union123 = df_VF_union12.union(df_VF_3)\n",
    "    df_VF_union1234 = df_VF_union123.union(df_VF_4)\n",
    "    \n",
    "    df_VF_init = df_VF_union1234.groupBy('r1').agg(collect_set('f1').alias('multi_f1'), collect_set('f2').alias('multi_f2'), collect_set('f3').alias('multi_f3'))\n",
    "    return df_VF_init\n",
    "\n",
    "df_VF_init = get_VF_init(df_tetra_order)\n",
    "\n",
    "# df_VE_init.cache()\n",
    "\n",
    "df_VF_init.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "infinite-tulsa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Ver: integer (nullable = true)\n",
      " |-- VF: array (nullable = true)\n",
      " |    |-- element: array (containsNull = true)\n",
      " |    |    |-- element: integer (containsNull = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# obtain VF relation\n",
    "def get_VF(multi_f1, multi_f2, multi_f3):\n",
    "# get_VF is used to obtain a complete VF relation from the partial VF relations\n",
    "# multi_f1: partial VF relation\n",
    "# multi_f2: partial VF relation\n",
    "# multi_f3: partial VF relation\n",
    "\n",
    "    faces = set()\n",
    "    for f in multi_f1:\n",
    "        faces.add(tuple(f))\n",
    "        \n",
    "    for f in multi_f2:\n",
    "        faces.add(tuple(f))\n",
    "        \n",
    "    for f in multi_f3:\n",
    "        faces.add(tuple(f))\n",
    "    \n",
    "    faces_list = sorted(faces) # save more time when using list(faces)\n",
    "    \n",
    "    return faces_list\n",
    "\n",
    "# convert a function to an udf and determine the return type\n",
    "# https://spark.apache.org/docs/3.1.3/api/python/reference/api/pyspark.sql.functions.udf.html\n",
    "get_VF_udf = udf(get_VF, ArrayType(ArrayType(IntegerType())))\n",
    "\n",
    "df_VF = df_VF_init.withColumn(\"VF\", get_VF_udf(df_VF_init.multi_f1, df_VF_init.multi_f2, df_VF_init.multi_f3)).drop('multi_f1', 'multi_f2', 'multi_f3')\n",
    "df_VF = df_VF.withColumnRenamed('r1', 'Ver')\n",
    "\n",
    "df_VF.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "modern-spokesman",
   "metadata": {},
   "source": [
    "##### join EV and VF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "assured-evening",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_EF_init_1 = df_EV.join(df_VF, df_EV.r1==df_VF.Ver).select(\"edge\", col(\"VF\").alias(\"VF_1\"), \"r2\")\n",
    "df_EF_init_2 = df_EF_init_1.join(df_VF, df_EF_init_1.r2==df_VF.Ver).select(\"edge\", \"VF_1\", col(\"VF\").alias(\"VF_2\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "final-papua",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_EF(edge, VF_1, VF_2):\n",
    "    EF = set()\n",
    "    edge_set = set(edge)\n",
    "    for face in VF_1:\n",
    "        face_set = set(face)\n",
    "        if edge_set.issubset(face_set):\n",
    "            ET.add(tuple(face))\n",
    "            \n",
    "    for face in VF_2:\n",
    "        face_set = set(face)\n",
    "        if edge_set.issubset(face_set):\n",
    "            ET.add(tuple(face))\n",
    "            \n",
    "    EF_list = list(EF)\n",
    "    \n",
    "    return EF_list\n",
    "\n",
    "get_EF_udf = udf(get_EF, ArrayType(ArrayType(IntegerType())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fundamental-medicare",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- edge: array (nullable = false)\n",
      " |    |-- element: integer (containsNull = true)\n",
      " |-- EF: array (nullable = true)\n",
      " |    |-- element: array (containsNull = true)\n",
      " |    |    |-- element: integer (containsNull = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_EF = df_EF_init_2.withColumn(\"EF\", get_EF_udf(df_EF_init_2.edge, df_EF_init_2.VF_1, df_EF_init_2.VF_2)).select(\"edge\", \"EF\")\n",
    "df_EF.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "intermediate-regular",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time cost: 1628.194329738617\n",
      "number of rows: 11511582639\n"
     ]
    }
   ],
   "source": [
    "t0 = time.time()\n",
    "\n",
    "num_row = df_EF.count()\n",
    "\n",
    "t1 = time.time()\n",
    "print(\"Time cost:\", t1 - t0)\n",
    "print(\"number of rows:\", num_row)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "under-intermediate",
   "metadata": {},
   "source": [
    "### Method 2: symmetric global (get EF from FE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "opened-consciousness",
   "metadata": {},
   "source": [
    "##### load the pre-computed EF relation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "widespread-prospect",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read a parquet file from hdfs\n",
    "file_FE = directory + '/' + tin_filename + '_FE.parquet'\n",
    "\n",
    "df_FE = spark.read.parquet(file_FE)\n",
    "df_FE.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "spread-webcam",
   "metadata": {},
   "outputs": [],
   "source": [
    "t0 = time.time()\n",
    "\n",
    "num_row = df_FE.count()\n",
    "\n",
    "t1 = time.time()\n",
    "print(\"Time cost:\", t1 - t0)\n",
    "print(\"number of rows:\", num_row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bigger-covering",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_EF_1 = df_FE.select('e1', 'face')\n",
    "df_EF_2 = df_FE.select('e2', 'face')\n",
    "df_EF_3 = df_FE.select('e3', 'face')\n",
    "\n",
    "df_EF_12 = df_EF_1.union(df_EF_2)\n",
    "df_EF_123 = df_EF_12.union(df_EF_3)\n",
    "\n",
    "df_EF = df_EF_123.groupBy('e1').agg(collect_set('face').alias('EF'))\n",
    "df_EF = df_EF.withColumnRenamed('e1', 'Edge')\n",
    "\n",
    "df_EF.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "worthy-jurisdiction",
   "metadata": {},
   "outputs": [],
   "source": [
    "t0 = time.time()\n",
    "\n",
    "num_row = df_EF.count()\n",
    "\n",
    "t1 = time.time()\n",
    "print(\"Time cost:\", t1 - t0)\n",
    "print(\"number of rows:\", num_row)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "opened-merchandise",
   "metadata": {},
   "source": [
    "### Method 3: local method (get EF from VT)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "federal-retreat",
   "metadata": {},
   "source": [
    "##### load the pre-computed VT relation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "crazy-probability",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Ver: integer (nullable = true)\n",
      " |-- VT: array (nullable = true)\n",
      " |    |-- element: array (containsNull = true)\n",
      " |    |    |-- element: integer (containsNull = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# read a parquet file from hdfs\n",
    "file_VT = directory + '/' + tin_filename + '_VT.parquet'\n",
    "\n",
    "df_VT = spark.read.parquet(file_VT)\n",
    "df_VT.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "specific-withdrawal",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time cost: 8.718859910964966\n",
      "number of rows: 224932406\n"
     ]
    }
   ],
   "source": [
    "t0 = time.time()\n",
    "\n",
    "num_row = df_VT.count()\n",
    "\n",
    "t1 = time.time()\n",
    "print(\"Time cost:\", t1 - t0)\n",
    "print(\"number of rows:\", num_row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "collaborative-rebel",
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import combinations\n",
    "def get_EF_from_VT(Ver, VT):\n",
    "    if VT:\n",
    "        multi_EF = {}\n",
    "        for tretra in VT:\n",
    "            # Generate combinations of 3 elements including Ver\n",
    "            for face in combinations(tretra, 3):\n",
    "                if Ver in face:\n",
    "                    sorted_face = sorted(face, reverse=True)\n",
    "                    if Ver == sorted_face[0]:\n",
    "                        # add the EF of first edge [sorted_face[0], sorted_face[1]]\n",
    "                        if tuple([sorted_face[0], sorted_face[1]]) in multi_EF:\n",
    "                            if sorted_face not in multi_EF[tuple([sorted_face[0], sorted_face[1]])]:\n",
    "                                multi_EF[tuple([sorted_face[0], sorted_face[1]])].append(sorted_face)\n",
    "                        else:\n",
    "                            multi_EF[tuple([sorted_face[0], sorted_face[1]])] = [sorted_face]\n",
    "                            \n",
    "                        # add the EF of second edge [sorted_face[0], sorted_face[2]]\n",
    "                        if tuple([sorted_face[0], sorted_face[2]]) in multi_EF:\n",
    "                            if sorted_face not in multi_EF[tuple([sorted_face[0], sorted_face[2]])]:\n",
    "                                multi_EF[tuple([sorted_face[0], sorted_face[2]])].append(sorted_face)\n",
    "                        else:\n",
    "                            multi_EF[tuple([sorted_face[0], sorted_face[2]])] = [sorted_face]\n",
    "                            \n",
    "                    elif Ver == sorted_face[1]:\n",
    "                        # add the EF of first edge [sorted_face[0], sorted_face[1]]\n",
    "                        if tuple([sorted_face[0], sorted_face[1]]) in multi_EF:\n",
    "                            if sorted_face not in multi_EF[tuple([sorted_face[0], sorted_face[1]])]:\n",
    "                                multi_EF[tuple([sorted_face[0], sorted_face[1]])].append(sorted_face)\n",
    "                        else:\n",
    "                            multi_EF[tuple([sorted_face[0], sorted_face[1]])] = [sorted_face]\n",
    "                            \n",
    "                        # add the EF of second edge [sorted_face[1], sorted_face[2]]\n",
    "                        if tuple([sorted_face[1], sorted_face[2]]) in multi_EF:\n",
    "                            if sorted_face not in multi_EF[tuple([sorted_face[1], sorted_face[2]])]:\n",
    "                                multi_EF[tuple([sorted_face[1], sorted_face[2]])].append(sorted_face)\n",
    "                        else:\n",
    "                            multi_EF[tuple([sorted_face[1], sorted_face[2]])] = [sorted_face]\n",
    "                            \n",
    "                    elif Ver == sorted_face[2]:\n",
    "                        # add the EF of first edge [sorted_face[0], sorted_face[2]]\n",
    "                        if tuple([sorted_face[0], sorted_face[2]]) in multi_EF:\n",
    "                            if sorted_face not in multi_EF[tuple([sorted_face[0], sorted_face[2]])]:\n",
    "                                multi_EF[tuple([sorted_face[0], sorted_face[2]])].append(sorted_face)\n",
    "                        else:\n",
    "                            multi_EF[tuple([sorted_face[0], sorted_face[2]])] = [sorted_face]\n",
    "                            \n",
    "                        # add the EF of second edge [sorted_face[1], sorted_face[2]]\n",
    "                        if tuple([sorted_face[1], sorted_face[2]]) in multi_EF:\n",
    "                            if sorted_face not in multi_EF[tuple([sorted_face[1], sorted_face[2]])]:\n",
    "                                multi_EF[tuple([sorted_face[1], sorted_face[2]])].append(sorted_face)\n",
    "                        else:\n",
    "                            multi_EF[tuple([sorted_face[1], sorted_face[2]])] = [sorted_face]\n",
    "                            \n",
    "        return multi_EF\n",
    "             \n",
    "get_EF_from_VT_udf = udf(get_EF_from_VT, MapType(ArrayType(IntegerType()), ArrayType(ArrayType(IntegerType()))))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "increased-crash",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Ver: integer (nullable = true)\n",
      " |-- EF: map (nullable = true)\n",
      " |    |-- key: array\n",
      " |    |    |-- element: integer (containsNull = true)\n",
      " |    |-- value: array (valueContainsNull = true)\n",
      " |    |    |-- element: array (containsNull = true)\n",
      " |    |    |    |-- element: integer (containsNull = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_EF_init = df_VT.withColumn(\"EF\", get_EF_from_VT_udf(df_VT.Ver, df_VT.VT))\n",
    "df_EF = df_EF_init.select('Ver', 'EF')\n",
    "# df_EF = df_EF_init.select(explode(df_EF_init.EF).alias(\"Edge\", \"EF\"))\n",
    "\n",
    "# df_EF = df_EF.dropDuplicates([\"Edge\"])\n",
    "df_EF.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "painted-count",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time cost: 4.521254539489746\n",
      "number of rows: 224932406\n"
     ]
    }
   ],
   "source": [
    "t0 = time.time()\n",
    "\n",
    "num_row = df_EF.count()\n",
    "\n",
    "t1 = time.time()\n",
    "print(\"Time cost:\", t1 - t0)\n",
    "print(\"number of rows:\", num_row)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecological-terrorist",
   "metadata": {},
   "source": [
    "# obtain ET relation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "worst-owner",
   "metadata": {},
   "source": [
    "### Method 1: pure global"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "classical-virginia",
   "metadata": {},
   "source": [
    "##### first get EV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "labeled-commander",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to get TE relation\n",
    "def get_TE_from_T(df_tetra_order):\n",
    "    df_TE = df_tetra_order.withColumn(\"tetra\", sort_array(F.array(\"r1\", \"r2\", \"r3\", \"r4\"), False)).withColumn(\"e1\", sort_array(F.array(\"r1\", \"r2\",), False)).withColumn(\"e2\", sort_array(F.array(\"r1\", \"r3\",), False)).withColumn(\"e3\", sort_array(F.array(\"r1\", \"r4\",), False)).withColumn(\"e4\", sort_array(F.array(\"r2\", \"r3\",), False)).withColumn(\"e5\", sort_array(F.array(\"r2\", \"r4\",), False)).withColumn(\"e6\", sort_array(F.array(\"r3\", \"r4\",), False))\n",
    "    df_TE = df_TE.select('tetra', 'e1', 'e2', 'e3', 'e4', 'e5', 'e6')\n",
    "    \n",
    "    return df_TE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "important-rebecca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- edge: array (nullable = false)\n",
      " |    |-- element: integer (containsNull = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# get edges from DF_T\n",
    "\n",
    "df_TE = get_TE_from_T(df_tetra_order)\n",
    "\n",
    "df_E1 = df_TE.select('e1')\n",
    "df_E2 = df_TE.select('e2')\n",
    "df_E3 = df_TE.select('e3')\n",
    "df_E4 = df_TE.select('e4')\n",
    "df_E5 = df_TE.select('e5')\n",
    "df_E6 = df_TE.select('e6')\n",
    "\n",
    "df_Edges = df_E1.union(df_E2).union(df_E3).union(df_E4).union(df_E5).union(df_E6)\n",
    "df_Edges = df_Edges.withColumnRenamed('e1', 'edge')\n",
    "df_Edges = df_Edges.distinct()\n",
    "df_Edges.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "wicked-encoding",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- edge: array (nullable = false)\n",
      " |    |-- element: integer (containsNull = true)\n",
      " |-- r1: integer (nullable = true)\n",
      " |-- r2: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_EV = df_Edges.withColumn('r1', df_Edges.edge[0]).withColumn('r2', df_Edges.edge[1])\n",
    "df_EV.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rental-companion",
   "metadata": {},
   "source": [
    "##### then get VT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "gothic-technique",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to get VT directly from DF_T\n",
    "def get_VT(df_tetra_order):\n",
    "    df_tetra_order = df_tetra_order.withColumn(\"tetra\", sort_array(F.array(\"r1\", \"r2\", \"r3\", \"r4\"), False))\n",
    "    df_VT_init_1 = df_tetra_order.select(\"r1\",\"tetra\")\n",
    "    df_VT_init_2 = df_tetra_order.select(\"r2\",\"tetra\")\n",
    "    df_VT_init_3 = df_tetra_order.select(\"r3\",\"tetra\")\n",
    "    df_VT_init_4 = df_tetra_order.select(\"r4\",\"tetra\")\n",
    "    \n",
    "    df_VT_union12 = df_VT_init_1.union(df_VT_init_2)\n",
    "    df_VT_union123 = df_VT_union12.union(df_VT_init_3)\n",
    "    df_VT_union1234 = df_VT_union123.union(df_VT_init_4)\n",
    "    \n",
    "    df_VT = df_VT_union1234.groupBy('r1').agg(collect_list('tetra').alias('VT'))\n",
    "    df_VT = df_VT.withColumnRenamed('r1', 'Ver')\n",
    "    \n",
    "    return df_VT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "minus-third",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Ver: integer (nullable = true)\n",
      " |-- VT: array (nullable = true)\n",
      " |    |-- element: array (containsNull = true)\n",
      " |    |    |-- element: integer (containsNull = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_VT = get_VT(df_tetra_order)\n",
    "\n",
    "df_VT.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hindu-affiliation",
   "metadata": {},
   "source": [
    "##### join df_EV and df_VT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "destroyed-things",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ET_init_1 = df_EV.join(df_VT, df_EV.r1==df_VT.Ver).select(\"edge\", col(\"VT\").alias(\"VT_1\"), \"r2\")\n",
    "df_ET_init_2 = df_ET_init_1.join(df_VT, df_ET_init_1.r2==df_VT.Ver).select(\"edge\", \"VT_1\", col(\"VT\").alias(\"VT_2\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "polished-holiday",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ET(edge, VT_1, VT_2):\n",
    "    ET = set()\n",
    "    edge_set = set(edge)\n",
    "    for tetra in VT_1:\n",
    "        tetra_set = set(tetra)\n",
    "        if edge_set.issubset(tetra_set):\n",
    "            ET.add(tuple(tetra))\n",
    "            \n",
    "    for tetra in VT_2:\n",
    "        tetra_set = set(tetra)\n",
    "        if edge_set.issubset(tetra_set):\n",
    "            ET.add(tuple(tetra))\n",
    "            \n",
    "    ET_list = list(ET)\n",
    "    \n",
    "    return ET_list\n",
    "\n",
    "get_ET_udf = udf(get_ET, ArrayType(ArrayType(IntegerType())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "charged-break",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- edge: array (nullable = false)\n",
      " |    |-- element: integer (containsNull = true)\n",
      " |-- ET: array (nullable = true)\n",
      " |    |-- element: array (containsNull = true)\n",
      " |    |    |-- element: integer (containsNull = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_ET = df_ET_init_2.withColumn(\"ET\", get_ET_udf(df_ET_init_2.edge, df_ET_init_2.VT_1, df_ET_init_2.VT_2)).select(\"edge\", \"ET\")\n",
    "df_ET.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "democratic-mount",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time cost: 1649.0040829181671\n",
      "number of rows: 11511582639\n"
     ]
    }
   ],
   "source": [
    "t0 = time.time()\n",
    "\n",
    "num_row = df_ET.count()\n",
    "\n",
    "t1 = time.time()\n",
    "print(\"Time cost:\", t1 - t0)\n",
    "print(\"number of rows:\", num_row)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wooden-diving",
   "metadata": {},
   "source": [
    "### Method 2: symmetric global (get ET from TE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "experimental-satellite",
   "metadata": {},
   "source": [
    "##### load the pre-computed TE relation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "generic-evaluation",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- tetra: array (nullable = true)\n",
      " |    |-- element: integer (containsNull = true)\n",
      " |-- e1: array (nullable = true)\n",
      " |    |-- element: integer (containsNull = true)\n",
      " |-- e2: array (nullable = true)\n",
      " |    |-- element: integer (containsNull = true)\n",
      " |-- e3: array (nullable = true)\n",
      " |    |-- element: integer (containsNull = true)\n",
      " |-- e4: array (nullable = true)\n",
      " |    |-- element: integer (containsNull = true)\n",
      " |-- e5: array (nullable = true)\n",
      " |    |-- element: integer (containsNull = true)\n",
      " |-- e6: array (nullable = true)\n",
      " |    |-- element: integer (containsNull = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# read a parquet file from hdfs\n",
    "file_TE = directory + '/' + tin_filename + '_TE.parquet'\n",
    "\n",
    "df_TE = spark.read.parquet(file_TE)\n",
    "df_TE.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "stone-bonus",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time cost: 22.687546491622925\n",
      "number of rows: 9715202480\n"
     ]
    }
   ],
   "source": [
    "t0 = time.time()\n",
    "\n",
    "num_row = df_TE.count()\n",
    "\n",
    "t1 = time.time()\n",
    "print(\"Time cost:\", t1 - t0)\n",
    "print(\"number of rows:\", num_row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "accepting-timber",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Edge: array (nullable = true)\n",
      " |    |-- element: integer (containsNull = true)\n",
      " |-- ET: array (nullable = true)\n",
      " |    |-- element: array (containsNull = true)\n",
      " |    |    |-- element: integer (containsNull = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_ET_1 = df_TE.select('e1', 'tetra')\n",
    "df_ET_2 = df_TE.select('e2', 'tetra')\n",
    "df_ET_3 = df_TE.select('e3', 'tetra')\n",
    "df_ET_4 = df_TE.select('e4', 'tetra')\n",
    "df_ET_5 = df_TE.select('e5', 'tetra')\n",
    "df_ET_6 = df_TE.select('e6', 'tetra')\n",
    "\n",
    "df_ET_union = df_ET_1.union(df_ET_2).union(df_ET_3).union(df_ET_4).union(df_ET_5).union(df_ET_6)\n",
    "\n",
    "df_ET = df_ET_union.groupby('e1').agg(collect_set('tetra').alias('ET'))\n",
    "df_ET = df_ET.withColumnRenamed('e1', 'Edge')\n",
    "df_ET.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "alike-metropolitan",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time cost: 587.2726020812988\n",
      "number of rows: 11511582639\n"
     ]
    }
   ],
   "source": [
    "t0 = time.time()\n",
    "\n",
    "num_row = df_ET.count()\n",
    "\n",
    "t1 = time.time()\n",
    "print(\"Time cost:\", t1 - t0)\n",
    "print(\"number of rows:\", num_row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "practical-facing",
   "metadata": {},
   "outputs": [],
   "source": [
    "t0 = time.time()\n",
    "\n",
    "file_ET = directory + '/' + tin_filename + '_ET.parquet'\n",
    "df_ET.write.parquet(file_ET)\n",
    "\n",
    "t1 = time.time()\n",
    "print(\"Time cost:\", t1 - t0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "engaged-instrument",
   "metadata": {},
   "source": [
    "### Method 3: local method (get ET from VT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "starting-portugal",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Ver: integer (nullable = true)\n",
      " |-- VT: array (nullable = true)\n",
      " |    |-- element: array (containsNull = true)\n",
      " |    |    |-- element: integer (containsNull = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# read a parquet file from hdfs\n",
    "file_VT = directory + '/' + tin_filename + '_VT.parquet'\n",
    "\n",
    "df_VT = spark.read.parquet(file_VT)\n",
    "df_VT.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "uniform-yugoslavia",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time cost: 18.245393753051758\n",
      "number of rows: 1792989718\n"
     ]
    }
   ],
   "source": [
    "t0 = time.time()\n",
    "\n",
    "num_row = df_VT.count()\n",
    "\n",
    "t1 = time.time()\n",
    "print(\"Time cost:\", t1 - t0)\n",
    "print(\"number of rows:\", num_row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "saved-controversy",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ET_from_VT_star(Ver, VT):\n",
    "    ET = {}\n",
    "    for tretra in VT:\n",
    "        if Ver in tretra:\n",
    "            index_ver = tretra.index(Ver)\n",
    "            if index_ver == 0: # the first vertex is the expected one\n",
    "                e_0 = [tretra[0], tretra[1]]\n",
    "                if tuple(e_0) in ET:\n",
    "                    ET[tuple(e_0)].append(tretra)\n",
    "                else:\n",
    "                    ET[tuple(e_0)] = [tretra]\n",
    "                    \n",
    "                e_1 = [tretra[0], tretra[2]]\n",
    "                if tuple(e_1) in ET:\n",
    "                    ET[tuple(e_1)].append(tretra)\n",
    "                else:\n",
    "                    ET[tuple(e_1)] = [tretra]\n",
    "                    \n",
    "                e_2 = [tretra[0], tretra[3]]\n",
    "                if tuple(e_2) in ET:\n",
    "                    ET[tuple(e_2)].append(tretra)\n",
    "                else:\n",
    "                    ET[tuple(e_2)] = [tretra]\n",
    "                    \n",
    "            elif index_ver == 1: # the second vertex is the expected one\n",
    "                e_0 = [tretra[1], tretra[2]]\n",
    "                if tuple(e_0) in ET:\n",
    "                    ET[tuple(e_0)].append(tretra)\n",
    "                else:\n",
    "                    ET[tuple(e_0)] = [tretra]\n",
    "                    \n",
    "                e_1 = [tretra[1], tretra[3]]\n",
    "                if tuple(e_1) in ET:\n",
    "                    ET[tuple(e_1)].append(tretra)\n",
    "                else:\n",
    "                    ET[tuple(e_1)] = [tretra]\n",
    "                    \n",
    "            elif index_ver == 2: # the second vertex is the expected one\n",
    "                e_0 = [tretra[2], tretra[3]]\n",
    "                if tuple(e_0) in ET:\n",
    "                    ET[tuple(e_0)].append(tretra)\n",
    "                else:\n",
    "                    ET[tuple(e_0)] = [tretra]\n",
    "                    \n",
    "    return ET\n",
    "\n",
    "get_ET_from_VT_star_udf = udf(get_ET_from_VT_star, MapType(ArrayType(IntegerType()), ArrayType(ArrayType(IntegerType()))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "blessed-phone",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Ver: integer (nullable = true)\n",
      " |-- ET: map (nullable = true)\n",
      " |    |-- key: array\n",
      " |    |    |-- element: integer (containsNull = true)\n",
      " |    |-- value: array (valueContainsNull = true)\n",
      " |    |    |-- element: array (containsNull = true)\n",
      " |    |    |    |-- element: integer (containsNull = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# store the FT relation within VT star\n",
    "df_ET = df_VT.withColumn(\"ET\", get_ET_from_VT_star_udf(df_VT.Ver, df_VT.VT))\n",
    "df_ET = df_ET.select(\"Ver\", \"ET\")\n",
    "df_ET.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "threatened-perspective",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time cost: 6.183401346206665\n",
      "number of rows: 1792989718\n"
     ]
    }
   ],
   "source": [
    "t0 = time.time()\n",
    "\n",
    "num_row = df_ET.count()\n",
    "\n",
    "t1 = time.time()\n",
    "print(\"Time cost:\", t1 - t0)\n",
    "print(\"number of rows:\", num_row)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "headed-runner",
   "metadata": {},
   "source": [
    "# obtain FT relation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "public-cursor",
   "metadata": {},
   "source": [
    "### Method 1: pure global"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "practical-heating",
   "metadata": {},
   "source": [
    "##### first get FV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "romance-newcastle",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to get TF relation\n",
    "def get_TF_from_T(df_tetra_order):\n",
    "    df_TF = df_tetra_order.withColumn(\"tetra\", sort_array(F.array(\"r1\", \"r2\", \"r3\", \"r4\"), False)).withColumn(\"f1\", sort_array(F.array(\"r1\", \"r2\", \"r3\"), False)).withColumn(\"f2\", sort_array(F.array(\"r1\", \"r2\", \"r4\"), False)).withColumn(\"f3\", sort_array(F.array(\"r1\", \"r3\", \"r4\"), False)).withColumn(\"f4\", sort_array(F.array(\"r2\", \"r3\", \"r4\"), False))\n",
    "    df_TF = df_TF.select('tetra', 'f1', 'f2', 'f3', 'f4')\n",
    "    \n",
    "    return df_TF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "threatened-office",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- face: array (nullable = false)\n",
      " |    |-- element: integer (containsNull = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# get faces from DF_T\n",
    "\n",
    "df_TF = get_TF_from_T(df_tetra_order)\n",
    "\n",
    "df_F1 = df_TF.select('f1')\n",
    "df_F2 = df_TF.select('f2')\n",
    "df_F3 = df_TF.select('f3')\n",
    "df_F4 = df_TF.select('f4')\n",
    "\n",
    "df_Faces = df_F1.union(df_F2).union(df_F3).union(df_F4)\n",
    "df_Faces = df_Faces.withColumnRenamed('f1', 'face')\n",
    "df_Faces = df_Faces.distinct()\n",
    "df_Faces.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "recognized-florist",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- face: array (nullable = false)\n",
      " |    |-- element: integer (containsNull = true)\n",
      " |-- r1: integer (nullable = true)\n",
      " |-- r2: integer (nullable = true)\n",
      " |-- r3: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_FV = df_Faces.withColumn('r1', df_Faces.face[0]).withColumn('r2', df_Faces.face[1]).withColumn('r3', df_Faces.face[2])\n",
    "df_FV.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "structural-practice",
   "metadata": {},
   "source": [
    "##### then get VT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "regional-ridge",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to get VT directly from DF_T\n",
    "def get_VT(df_tetra_order):\n",
    "    df_tetra_order = df_tetra_order.withColumn(\"tetra\", sort_array(F.array(\"r1\", \"r2\", \"r3\", \"r4\"), False))\n",
    "    df_VT_init_1 = df_tetra_order.select(\"r1\",\"tetra\")\n",
    "    df_VT_init_2 = df_tetra_order.select(\"r2\",\"tetra\")\n",
    "    df_VT_init_3 = df_tetra_order.select(\"r3\",\"tetra\")\n",
    "    df_VT_init_4 = df_tetra_order.select(\"r4\",\"tetra\")\n",
    "    \n",
    "    df_VT_union12 = df_VT_init_1.union(df_VT_init_2)\n",
    "    df_VT_union123 = df_VT_union12.union(df_VT_init_3)\n",
    "    df_VT_union1234 = df_VT_union123.union(df_VT_init_4)\n",
    "    \n",
    "    df_VT = df_VT_union1234.groupBy('r1').agg(collect_list('tetra').alias('VT'))\n",
    "    df_VT = df_VT.withColumnRenamed('r1', 'Ver')\n",
    "    \n",
    "    return df_VT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "piano-connecticut",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Ver: integer (nullable = true)\n",
      " |-- VT: array (nullable = true)\n",
      " |    |-- element: array (containsNull = true)\n",
      " |    |    |-- element: integer (containsNull = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_VT = get_VT(df_tetra_order)\n",
    "\n",
    "df_VT.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fossil-translator",
   "metadata": {},
   "source": [
    "##### join df_FV and df_VT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "swiss-companion",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_FT_init_1 = df_FV.join(df_VT, df_FV.r1==df_VT.Ver).select(\"face\", col(\"VT\").alias(\"VT_1\"), \"r2\", \"r3\")\n",
    "df_FT_init_2 = df_FT_init_1.join(df_VT, df_FT_init_1.r2==df_VT.Ver).select(\"face\", \"VT_1\", col(\"VT\").alias(\"VT_2\"), \"r3\")\n",
    "df_FT_init_3 = df_FT_init_2.join(df_VT, df_FT_init_2.r3==df_VT.Ver).select(\"face\", \"VT_1\", \"VT_2\", col(\"VT\").alias(\"VT_3\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "musical-promotion",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_FT(face, VT_1, VT_2, VT_3):\n",
    "    FT = set()\n",
    "    face_set = set(face)\n",
    "    for tetra in VT_1:\n",
    "        tetra_set = set(tetra)\n",
    "        if face_set.issubset(tetra_set):\n",
    "            FT.add(tuple(tetra))\n",
    "            \n",
    "    for tetra in VT_2:\n",
    "        tetra_set = set(tetra)\n",
    "        if face_set.issubset(tetra_set):\n",
    "            FT.add(tuple(tetra))\n",
    "            \n",
    "    for tetra in VT_3:\n",
    "        tetra_set = set(tetra)\n",
    "        if face_set.issubset(tetra_set):\n",
    "            FT.add(tuple(tetra))\n",
    "            \n",
    "    FT_list = list(FT)\n",
    "    \n",
    "    return FT_list\n",
    "\n",
    "get_FT_udf = udf(get_FT, ArrayType(ArrayType(IntegerType())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "happy-relaxation",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- face: array (nullable = false)\n",
      " |    |-- element: integer (containsNull = true)\n",
      " |-- FT: array (nullable = true)\n",
      " |    |-- element: array (containsNull = true)\n",
      " |    |    |-- element: integer (containsNull = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_FT = df_FT_init_3.withColumn(\"FT\", get_FT_udf(df_FT_init_3.face, df_FT_init_3.VT_1, df_FT_init_3.VT_2, df_FT_init_3.VT_3)).select(\"face\", \"FT\")\n",
    "df_FT.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "nervous-decimal",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time cost: 1778.6693172454834\n",
      "number of rows: 19434269798\n"
     ]
    }
   ],
   "source": [
    "t0 = time.time()\n",
    "\n",
    "num_row = df_FT.count()\n",
    "\n",
    "t1 = time.time()\n",
    "print(\"Time cost:\", t1 - t0)\n",
    "print(\"number of rows:\", num_row)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "demonstrated-creek",
   "metadata": {},
   "source": [
    "### Method 2: symmetric global (get FT from TF)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "nearby-final",
   "metadata": {},
   "source": [
    "##### load the pre-computed TF relation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "central-incentive",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read a parquet file from hdfs\n",
    "file_TF = directory + '/' + tin_filename + '_TF.parquet'\n",
    "\n",
    "df_TF = spark.read.parquet(file_TF)\n",
    "df_TF.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sealed-zoning",
   "metadata": {},
   "outputs": [],
   "source": [
    "t0 = time.time()\n",
    "\n",
    "num_row = df_TF.count()\n",
    "\n",
    "t1 = time.time()\n",
    "print(\"Time cost:\", t1 - t0)\n",
    "print(\"number of rows:\", num_row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "devoted-johns",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_FT_1 = df_TF.select('f1', 'tetra')\n",
    "df_FT_2 = df_TF.select('f2', 'tetra')\n",
    "df_FT_3 = df_TF.select('f3', 'tetra')\n",
    "df_FT_4 = df_TF.select('f4', 'tetra')\n",
    "\n",
    "df_FT_12 = df_FT_1.union(df_FT_2)\n",
    "df_FT_123 = df_FT_12.union(df_FT_3)\n",
    "df_FT_1234 = df_FT_123.union(df_FT_4)\n",
    "\n",
    "df_FT = df_FT_1234.groupBy('f1').agg(collect_set('tetra').alias('FT'))\n",
    "df_FT = df_FT.withColumnRenamed(\"f1\", \"Face\")\n",
    "df_FT.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "suited-tribute",
   "metadata": {},
   "outputs": [],
   "source": [
    "t0 = time.time()\n",
    "\n",
    "num_row = df_FT.count()\n",
    "\n",
    "t1 = time.time()\n",
    "print(\"Time cost:\", t1 - t0)\n",
    "print(\"number of rows:\", num_row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "loose-paste",
   "metadata": {},
   "outputs": [],
   "source": [
    "t0 = time.time()\n",
    "\n",
    "file_FT = directory + '/' + tin_filename + '_FT.parquet'\n",
    "df_FT.write.parquet(file_FT)\n",
    "\n",
    "t1 = time.time()\n",
    "print(\"Time cost:\", t1 - t0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "refined-ability",
   "metadata": {},
   "source": [
    "### Method 3: local method (get FT from VT)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "phantom-principle",
   "metadata": {},
   "source": [
    "##### load the pre-computed VT relation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "spiritual-chosen",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Ver: integer (nullable = true)\n",
      " |-- VT: array (nullable = true)\n",
      " |    |-- element: array (containsNull = true)\n",
      " |    |    |-- element: integer (containsNull = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# read a parquet file from hdfs\n",
    "file_VT = directory + '/' + tin_filename + '_VT.parquet'\n",
    "\n",
    "df_VT = spark.read.parquet(file_VT)\n",
    "df_VT.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "seasonal-threat",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time cost: 16.39375925064087\n",
      "number of rows: 1792989718\n"
     ]
    }
   ],
   "source": [
    "t0 = time.time()\n",
    "\n",
    "num_row = df_VT.count()\n",
    "\n",
    "t1 = time.time()\n",
    "print(\"Time cost:\", t1 - t0)\n",
    "print(\"number of rows:\", num_row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "automatic-thing",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_FT_from_VT_star(Ver, VT):\n",
    "    FT = {}\n",
    "    for tretra in VT:\n",
    "        if Ver in tretra:\n",
    "            index_ver = tretra.index(Ver)\n",
    "            if index_ver == 0: # the first vertex is the expected one\n",
    "                face_0 = [tretra[0], tretra[1], tretra[2]]\n",
    "                if tuple(face_0) in FT:\n",
    "                    FT[tuple(face_0)].append(tretra)\n",
    "                else:\n",
    "                    FT[tuple(face_0)] = [tretra]\n",
    "                    \n",
    "                face_1 = [tretra[0], tretra[1], tretra[3]]\n",
    "                if tuple(face_1) in FT:\n",
    "                    FT[tuple(face_1)].append(tretra)\n",
    "                else:\n",
    "                    FT[tuple(face_1)] = [tretra]\n",
    "                    \n",
    "                face_2 = [tretra[0], tretra[2], tretra[3]]\n",
    "                if tuple(face_2) in FT:\n",
    "                    FT[tuple(face_2)].append(tretra)\n",
    "                else:\n",
    "                    FT[tuple(face_2)] = [tretra]\n",
    "                    \n",
    "            elif index_ver == 1: # the second vertex is the expected one\n",
    "                face_0 = [tretra[1], tretra[2], tretra[3]]\n",
    "                if tuple(face_0) in FT:\n",
    "                    FT[tuple(face_0)].append(tretra)\n",
    "                else:\n",
    "                    FT[tuple(face_0)] = [tretra]\n",
    "                    \n",
    "    return FT\n",
    "\n",
    "get_FT_from_VT_star_udf = udf(get_FT_from_VT_star, MapType(ArrayType(IntegerType()), ArrayType(ArrayType(IntegerType()))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "angry-validation",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Ver: integer (nullable = true)\n",
      " |-- FT: map (nullable = true)\n",
      " |    |-- key: array\n",
      " |    |    |-- element: integer (containsNull = true)\n",
      " |    |-- value: array (valueContainsNull = true)\n",
      " |    |    |-- element: array (containsNull = true)\n",
      " |    |    |    |-- element: integer (containsNull = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# store the FT relation within VT star\n",
    "df_FT_star = df_VT.withColumn(\"FT\", get_FT_from_VT_star_udf(df_VT.Ver, df_VT.VT))\n",
    "df_FT = df_FT_star.select(\"Ver\", \"FT\")\n",
    "df_FT.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "perceived-developer",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time cost: 6.1875526905059814\n",
      "number of rows: 1792989718\n"
     ]
    }
   ],
   "source": [
    "t0 = time.time()\n",
    "\n",
    "num_row = df_FT.count()\n",
    "\n",
    "t1 = time.time()\n",
    "print(\"Time cost:\", t1 - t0)\n",
    "print(\"number of rows:\", num_row)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
